# ğŸ§  Toy LLaMA 3.2 - Character-Level LLM

A **toy implementation of the LLaMA 3.2 architecture** for learning purposes. This project trains a character-level language model using a simplified version of the LLaMA 3.2 transformer architecture. The model learns to generate Shakespeare-like text based on input prompts.

> âš ï¸ **Disclaimer**: This project is for **educational purposes only**. It is a minimal and experimental implementation. There might be inaccuracies, oversights, or suboptimal decisions. Please feel free to open issues or share suggestions!

---

## âœ¨ Features

* Character-level tokenizer
* Configurable transformer architecture (LLaMA 3.2-inspired)
* Grouped Query Attention + RoPE frequency interpolation
* Loss plotted and saved over training epochs
* Generation script with support for temperature sampling
* MPS (Mac) and CUDA GPU support with `autocast` context

---

## ğŸ“ˆ Training Results

Trained for **24 epochs** on Shakespeare dataset (char-level) using MPS device. Final training loss: **\~1.228**.

### ğŸ”¹ Training Output

<img src="assets/training.png" alt="Generated Text Example" width="600" />

### ğŸ”¹ Generated Text

<img src="assets/output.png" alt="Generated Text Example" width="600" />

### ğŸ”¹ Loss Curve

<img src="assets/loss_curve_24epochs.png" alt="Training Loss Curve" width="600" />

---

## ğŸ› ï¸ Usage

### 1. Train the model

```bash
uv run train.py
```

### 2. Generate text

```bash
uv run generate.py
```

---

## ğŸ“ Directory Structure

```
toy-llama3/
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ toy_llama/
â”‚       â””â”€â”€ layers.py       # Layer components
â”‚       â””â”€â”€ model.py        # Model architecture
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ loss_curve_24epochs.png
â”‚
â”œâ”€â”€ train.py                 # Training loop
â”œâ”€â”€ generate.py              # Text generation
â”œâ”€â”€ char_vocab.json          # Character-level vocab
â”œâ”€â”€ toy_llama3_24epochs.pth  # Trained model weights (gitignored)
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ§¾ Resources

This project was inspired and guided by:

* ğŸ“˜ Blog Post: *"The Big LLM Architecture Comparison"* â€“ \[https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison]
* ğŸ““ Jupyter Notebook: *"LLaMA 3.2 From Scratch"* â€“ \[https://github.com/rasbt/LLMs-from-scratch/blob/main/pkg/llms_from_scratch/llama3.py]

---

## ğŸ™ Acknowledgements

Thanks to the open-source community for resources and ideas. Special thanks to Meta and the LLaMA team for releasing their architecture and inspiring learning projects like this.

---

## ğŸ“Œ Notes

* The model weights (`*.pth`) are excluded from the repo.
* Results may vary based on training duration and device.
* The implementation prioritizes readability and learning over efficiency.